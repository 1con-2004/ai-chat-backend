import os
import time
import asyncio
import hashlib
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from openai import AsyncOpenAI
from dotenv import load_dotenv
import logging

load_dotenv()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="AIChatPartner Backend", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SuggestionRequest(BaseModel):
    message: str
    context: Optional[str] = None

class SuggestionResponse(BaseModel):
    suggestions: List[str]
    model_used: str
    response_time_ms: int

# ç¡…åŸºæµåŠ¨å®¢æˆ·ç«¯é…ç½®
silicon_client = AsyncOpenAI(
    api_key=os.getenv("SILICON_API_KEY", "sk-jpkihphqhqzxyyegluyiafujwphldkjvhkgpopfoynihjcvc"),
    base_url=os.getenv("SILICON_BASE_URL", "https://api.siliconflow.cn/v1")
)

AI_CONFIG = {
    "model": os.getenv("SILICON_MODEL", "Qwen/Qwen2.5-7B-Instruct"),
    "max_tokens": 120,
    "temperature": 0.8,
    "top_p": 0.9,
    "timeout": 8.0
}

CHAT_REPLY_PROMPT_TEMPLATE = """ä½ æ˜¯å¾®ä¿¡èŠå¤©åŠ©æ‰‹ã€‚ç”¨æˆ·æ”¶åˆ°æ¶ˆæ¯ï¼š"{message}"

ç”Ÿæˆ3æ¡ä¸åŒé£æ ¼çš„ä¸­æ–‡å›å¤ï¼š
1. æ­£å¼ç¤¼è²Œçš„å›å¤ï¼ˆ5-15å­—ï¼‰
2. è½»æ¾éšæ„çš„å›å¤ï¼ˆ5-15å­—ï¼‰
3. å¹½é»˜é£è¶£çš„å›å¤ï¼ˆ5-15å­—ï¼‰

è¦æ±‚ï¼š
- ç›´æ¥ç»™å‡ºå›å¤å†…å®¹ï¼Œä¸è¦ç¼–å·æˆ–è¯´æ˜
- æ¯è¡Œä¸€æ¡
- ç¬¦åˆä¸­æ–‡èŠå¤©ä¹ æƒ¯
- é£æ ¼å·®å¼‚æ˜æ˜¾

å›å¤ï¼š"""

async def generate_ai_suggestions(message: str, context: Optional[str] = None) -> List[str]:
    """è°ƒç”¨ç¡…åŸºæµåŠ¨APIç”Ÿæˆå›å¤å»ºè®®"""
    start_time = time.time()
    
    try:
        prompt = CHAT_REPLY_PROMPT_TEMPLATE.format(message=message)
        if context:
            prompt += f"\n[å¯¹è¯èƒŒæ™¯: {context[:100]}]"
        
        response = await asyncio.wait_for(
            silicon_client.chat.completions.create(
                model=AI_CONFIG["model"],
                messages=[{"role": "user", "content": prompt}],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=AI_CONFIG["temperature"],
                top_p=AI_CONFIG["top_p"]
            ),
            timeout=AI_CONFIG["timeout"]
        )
        
        content = response.choices[0].message.content.strip()
        suggestions = [
            line.strip() 
            for line in content.split('\n') 
            if line.strip() and len(line.strip()) > 1
        ]
        
        # è¿‡æ»¤å’ŒéªŒè¯å»ºè®®
        filtered_suggestions = []
        for suggestion in suggestions[:5]:
            if len(suggestion) <= 30 and not any(skip in suggestion for skip in ["1.", "2.", "3.", "å›å¤ï¼š", "å»ºè®®ï¼š"]):
                filtered_suggestions.append(suggestion)
        
        if len(filtered_suggestions) < 3:
            filtered_suggestions.extend(get_fallback_suggestions(message)[:3-len(filtered_suggestions)])
        
        elapsed_time = (time.time() - start_time) * 1000
        logger.info(f"AIç”Ÿæˆè€—æ—¶: {elapsed_time:.0f}ms, ç”Ÿæˆæ•°é‡: {len(filtered_suggestions)}")
        
        return filtered_suggestions[:3]
        
    except asyncio.TimeoutError:
        logger.warning("ç¡…åŸºæµåŠ¨APIè¶…æ—¶ï¼Œä½¿ç”¨æ™ºèƒ½å¤‡é€‰æ–¹æ¡ˆ")
        return get_smart_fallback_suggestions(message)
    except Exception as e:
        logger.error(f"ç¡…åŸºæµåŠ¨APIè°ƒç”¨å¤±è´¥: {e}")
        return get_smart_fallback_suggestions(message)

def get_smart_fallback_suggestions(message: str) -> List[str]:
    """æ™ºèƒ½å¤‡é€‰å»ºè®®ç”Ÿæˆ"""
    message_lower = message.lower()
    
    # åŸºäºå†…å®¹ç‰¹å¾çš„æ™ºèƒ½åŒ¹é…
    if any(word in message for word in ["è°¢è°¢", "æ„Ÿè°¢", "thank"]):
        return ["ä¸å®¢æ°”ğŸ˜Š", "åº”è¯¥çš„", "äº’ç›¸å¸®å¿™å˜›"]
    elif any(word in message for word in ["ä½ å¥½", "hi", "hello", "æ—©", "æ™šä¸Šå¥½"]):
        return ["ä½ å¥½", "å—¨", "æ—©ä¸Šå¥½å‘€"]
    elif any(word in message for word in ["ï¼Ÿ", "?", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "ä»€ä¹ˆ"]):
        return ["è®©æˆ‘æƒ³æƒ³", "è¿™ä¸ªé—®é¢˜ä¸é”™", "æœ‰é“ç†å“¦"]
    elif any(word in message for word in ["å“ˆå“ˆ", "ğŸ˜„", "ğŸ˜‚", "ç¬‘", "æœ‰è¶£"]):
        return ["å“ˆå“ˆå“ˆ", "ç¡®å®å¾ˆæœ‰æ„æ€", "ç¬‘æ­»æˆ‘äº†"]
    elif any(word in message for word in ["ç´¯", "å¿™", "è¾›è‹¦", "ç–²æƒ«"]):
        return ["æ³¨æ„ä¼‘æ¯", "åˆ«å¤ªç´¯äº†", "æ—©ç‚¹ç¡"]
    elif any(word in message for word in ["åƒ", "é¥­", "é£Ÿç‰©", "é¥¿"]):
        return ["æˆ‘ä¹Ÿé¥¿äº†", "å¥½åƒå—", "ä¸€èµ·åƒé¥­å§"]
    else:
        # åŸºäºæ¶ˆæ¯å“ˆå¸Œçš„ç¡®å®šæ€§å¤‡é€‰
        msg_hash = hashlib.md5(message.encode()).hexdigest()
        hash_index = int(msg_hash[:2], 16) % 5
        
        fallback_pools = [
            ["æ”¶åˆ°", "å¥½çš„", "æ˜ç™½äº†"],
            ["ç¡®å®", "æ˜¯çš„", "æœ‰é“ç†"],
            ["å—¯å—¯", "æ²¡é—®é¢˜", "OK"],
            ["ç†è§£", "çŸ¥é“äº†", "æ‡‚äº†"],
            ["å¥½å§", "è¿™æ ·å•Š", "åŸæ¥å¦‚æ­¤"]
        ]
        
        return fallback_pools[hash_index]

def get_fallback_suggestions(message: str) -> List[str]:
    """åŸºç¡€å¤‡é€‰å»ºè®®"""
    return ["å¥½çš„", "æ”¶åˆ°", "æ˜ç™½"]

@app.get("/")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "ok",
        "service": "AIChatPartner Backend",
        "model": AI_CONFIG["model"],
        "version": "1.0.0"
    }

@app.post("/getReplySuggestions", response_model=SuggestionResponse)
async def get_reply_suggestions(request: SuggestionRequest):
    """è·å–AIå›å¤å»ºè®®çš„ä¸»æ¥å£"""
    start_time = time.time()
    
    if not request.message or not request.message.strip():
        raise HTTPException(status_code=400, detail="æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    try:
        suggestions = await generate_ai_suggestions(request.message.strip(), request.context)
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return SuggestionResponse(
            suggestions=suggestions,
            model_used=AI_CONFIG["model"],
            response_time_ms=response_time_ms
        )
        
    except Exception as e:
        logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {e}")
        # é™çº§å¤„ç†ï¼šè¿”å›æ™ºèƒ½å¤‡é€‰å»ºè®®
        suggestions = get_smart_fallback_suggestions(request.message)
        response_time_ms = int((time.time() - start_time) * 1000)
        
        return SuggestionResponse(
            suggestions=suggestions,
            model_used="fallback",
            response_time_ms=response_time_ms
        )

@app.get("/health")
async def detailed_health():
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    try:
        # æµ‹è¯•APIè¿é€šæ€§
        test_start = time.time()
        response = await asyncio.wait_for(
            silicon_client.chat.completions.create(
                model=AI_CONFIG["model"],
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            ),
            timeout=5.0
        )
        test_time = (time.time() - test_start) * 1000
        
        return {
            "status": "healthy",
            "ai_service": "available",
            "model": AI_CONFIG["model"],
            "test_response_time_ms": int(test_time),
            "timestamp": int(time.time())
        }
    except Exception as e:
        return {
            "status": "degraded",
            "ai_service": "unavailable", 
            "error": str(e),
            "fallback_mode": True,
            "timestamp": int(time.time())
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7950)